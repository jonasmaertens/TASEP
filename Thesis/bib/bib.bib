@article{istrail_introduction_nodate,
	title = {Introduction to Markov Chains},
	author = {Istrail, Sorin},
	langid = {english},
	file = {Istrail - Introduction to Markov Chains.pdf:/Users/jonasprivat/Zotero/storage/XW9Y4MFG/Istrail - Introduction to Markov Chains.pdf:application/pdf},
}

@article{davies_introduction_nodate,
	title = {Introduction and Monte Carlo},
	author = {Davies, Robert},
	langid = {english},
	file = {Davies - Simulation - Lecture 1 - Introduction and Monte Ca.pdf:/Users/jonasprivat/Zotero/storage/YLA5LYD5/Davies - Simulation - Lecture 1 - Introduction and Monte Ca.pdf:application/pdf},
}

@online{ibm_nn,
	title = {What are Neural Networks? {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/neural-networks},
	shorttitle = {What are Neural Networks?},
	abstract = {Learn about neural networks that allow programs to recognize patterns and solve common problems in artificial intelligence, machine learning and deep learning.},
	urldate = {2023-11-16},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/E9M4TP2X/neural-networks.html:text/html},
}

@book{behrends_introduction_2000,
	location = {Wiesbaden},
	title = {Introduction to Markov Chains},
	isbn = {978-3-528-06986-5 978-3-322-90157-6},
	url = {http://link.springer.com/10.1007/978-3-322-90157-6},
	series = {Advanced Lectures in Mathematics},
	publisher = {Vieweg+Teubner Verlag},
	author = {Behrends, Ehrhard},
	urldate = {2023-11-16},
	date = {2000},
	doi = {10.1007/978-3-322-90157-6},
	keywords = {calculus, Counting, Finite, Markowsche Kette, Mathematische Statistik, Wahrscheinlichkeitstheorie},
	file = {Full Text:/Users/jonasprivat/Zotero/storage/5U7DF2LH/Behrends - 2000 - Introduction to Markov Chains.pdf:application/pdf},
}


@book{sutton_reinforcement_nodate,
	title = {Reinforcement Learning: An Introduction},
	author = {Sutton, Richard S and Barto, Andrew G},
	langid = {english},
	file = {Sutton and Barto - Reinforcement Learning An Introduction.pdf:/Users/jonasprivat/Zotero/storage/6CL3U8ZH/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf},
}


@online{harvard_ai,
	title = {The present and future of {AI}},
	url = {https://seas.harvard.edu/news/2021/10/present-and-future-ai},
	urldate = {2023-11-16},
	file = {The present and future of AI:/Users/jonasprivat/Zotero/storage/6RFKEZTK/present-and-future-ai.html:text/html},
}

@online{weforum_ai,
	title = {How has {AI} developed over the years and what's next?},
	url = {https://www.weforum.org/agenda/2022/12/how-ai-developed-whats-next-digital-transformation/},
	abstract = {Artificial intelligence has come a long way since the 1950s. We now have {AI} systems like {DALL}-E and {PaLM} with abilities to produce photorealistic images and interpret and generate language.},
	titleaddon = {World Economic Forum},
	urldate = {2023-11-16},
	date = {2022-12-12},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/4X5FLC3X/how-ai-developed-whats-next-digital-transformation.html:text/html},
}

@online{ibm_montecarlo,
	title = {What is Monte Carlo Simulation? {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/monte-carlo-simulation},
	shorttitle = {What is Monte Carlo Simulation?},
	abstract = {Learn everything you need to know about a Monte Carlo Simulation, a type of computational algorithm that uses repeated random sampling to obtain the likelihood of a range of results of occurring.},
	urldate = {2023-11-16},
	langid = {english},
}

@book{russell_artificial_2021,
	location = {Hoboken},
	edition = {Fourth edition},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	series = {Pearson series in artificial intelligence},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	date = {2021},
	langid = {english},
	keywords = {Artificial intelligence},
	file = {Russell and Norvig - 2021 - Artificial intelligence a modern approach.pdf:/Users/jonasprivat/Zotero/storage/ABYBGM5R/Russell and Norvig - 2021 - Artificial intelligence a modern approach.pdf:application/pdf},
}

@inreference{wiki_ai_2023,
	title = {Artificial intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1185402635},
	abstract = {Artificial intelligence ({AI}) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is also the field of study in computer science that develops and studies intelligent machines. "{AI}" may also refer to the machines themselves.
{AI} technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by {YouTube}, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools ({ChatGPT} and {AI} art), and competing at the highest level in strategy games (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous {AI} techniques, there was a vast increase in funding and interest.
The various sub-fields of {AI} research are centered around particular goals and the use of particular tools. The traditional goals of {AI} research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.
To solve these problems, {AI} researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. {AI} also draws upon psychology, linguistics, philosophy, neuroscience and many other fields.},
	booktitle = {Wikipedia},
	urldate = {2023-11-16},
	date = {2023-11-16},
	langid = {english},
	note = {Page Version {ID}: 1185402635},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/5R8VKPQ4/Artificial_intelligence.html:text/html},
}

@online{stanford-whatisai,
	title = {What is {AI}? / Basic Questions},
	url = {http://jmc.stanford.edu/artificial-intelligence/what-is-ai/index.html},
	urldate = {2023-11-16},
	file = {What is AI? / Basic Questions:/Users/jonasprivat/Zotero/storage/INAH9ER5/index.html:text/html},
}

@online{googletrends_ai,
	title = {Google Trends},
	url = {https://trends.google.com/trends/explore?date=today%205-y&q=%2Fm%2F0mkz},
	abstract = {Explore search interest for Artificial intelligence by time, location and popularity on Google Trends},
	titleaddon = {Google Trends},
	urldate = {2023-11-16},
	langid = {british},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/WGXRWQYB/explore.html:text/html},
}

@online{openai_chatgpt_intro,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/blog/chatgpt},
	abstract = {We’ve trained a model called {ChatGPT} which interacts in a conversational way. The dialogue format makes it possible for {ChatGPT} to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	urldate = {2023-11-16},
	langid = {american},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/2PV7KLDK/chatgpt.html:text/html},
}

@online{googlengram_ai,
	title = {Google Books Ngram Viewer},
	url = {https://books.google.com/ngrams/graph?content=AI&year_start=1800&year_end=2019&corpus=en-2019&smoothing=3},
	abstract = {Google Books Ngram Viewer},
	urldate = {2023-11-16},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/M3PVUJ3T/graph.html:text/html},
}

@online{mit_nnexplained,
	title = {Explained: Neural networks},
	url = {https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414},
	shorttitle = {Explained},
	abstract = {“Deep learning,” the machine-learning technique behind the best-performing artificial-intelligence systems of the past decade, is really a revival of the 70-year-old concept of neural networks.},
	titleaddon = {{MIT} News {\textbar} Massachusetts Institute of Technology},
	urldate = {2023-11-16},
	date = {2017-04-14},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/WTF7YGHG/explained-neural-networks-deep-learning-0414.html:text/html},
}

@inreference{wiki_machinelearning_2023,
	title = {Machine learning},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Machine_learning&oldid=1185036623},
	abstract = {Machine learning ({ML}) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of {ML} are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.{ML} is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.},
	booktitle = {Wikipedia},
	urldate = {2023-11-16},
	date = {2023-11-14},
	langid = {english},
	note = {Page Version {ID}: 1185036623},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/K2L2PD3W/Machine_learning.html:text/html},
}

@inreference{wiki_johnmccarthy_2023,
	title = {John {McCarthy} (computer scientist)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=John_McCarthy_(computer_scientist)&oldid=1184953817},
	abstract = {John {McCarthy} (September 4, 1927 – October 24, 2011) was an American computer scientist and cognitive scientist. He was one of the founders of the discipline of artificial intelligence. He co-authored the document that coined the term "artificial intelligence" ({AI}), developed the programming language family Lisp, significantly influenced the design of the language {ALGOL}, popularized time-sharing, and invented garbage collection.
{McCarthy} spent most of his career at Stanford University. He received many accolades and honors, such as the 1971 Turing Award for his contributions to the topic of {AI}, the United States National Medal of Science, and the Kyoto Prize.},
	booktitle = {Wikipedia},
	urldate = {2023-11-16},
	date = {2023-11-13},
	langid = {english},
	note = {Page Version {ID}: 1184953817},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/MRYQE7JA/John_McCarthy_(computer_scientist).html:text/html},
}

@online{woo_fatherofai_2014,
	title = {John {McCarthy} dies at 84; the father of artificial intelligence},
	url = {https://www.latimes.com/local/obituaries/la-me-john-mccarthy-20111027-story.html},
	abstract = {The mathematician, a longtime professor at Stanford, played a seminal role in defining the field devoted to the development of intelligent machines.},
	titleaddon = {Los Angeles Times},
	author = {Woo, Elaine},
	urldate = {2023-11-16},
	date = {2014-03-20},
	langid = {american},
	note = {Section: Obituaries},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/QKXWPFTL/la-me-john-mccarthy-20111027-story.html:text/html},
}

@online{homagejohnmccarthy,
	title = {John {McCarthy}: homage to the father of Artificial Intelli...},
	url = {https://www.teneo.ai/blog/homage-to-john-mccarthy-the-father-of-artificial-intelligence-ai},
	shorttitle = {John {McCarthy}},
	abstract = {Discover the fascinating story of John {McCarthy}, the father of...},
	titleaddon = {Teneo.Ai - Transforming every phone call to a love story with your brand},
	urldate = {2023-11-16},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/KSMD868A/homage-to-john-mccarthy-the-father-of-artificial-intelligence-ai.html:text/html},
}

@article{andresen_fatherofai_2002,
	title = {John {McCarthy}: Father of {AI}},
	volume = {17},
	issn = {1541-1672},
	url = {https://www.computer.org/csdl/magazine/ex/2002/05/x5084/13rRUxE04ph},
	doi = {10.1109/MIS.2002.1039837},
	shorttitle = {John {McCarthy}},
	abstract = {null},
	pages = {84--85},
	number = {5},
	journaltitle = {{IEEE} Intelligent Systems},
	author = {Andresen, Scott L.},
	urldate = {2023-11-16},
	date = {2002-09-01},
	note = {Publisher: {IEEE} Computer Society},
}

@online{mccarthy_proposal_1955,
	title = {A {PROPOSAL} {FOR} {THE} {DARTMOUTH} {SUMMER} {RESEARCH} {PROJECT} {ON} {ARTIFICIAL} {INTELLIGENCE}},
	url = {http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html},
	author = {McCarthy, John and Minsky, Marvin and Rochester, Nathan and Shannon, Claude},
	urldate = {2023-11-16},
	file = {A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE:/Users/jonasprivat/Zotero/storage/8R5NSQD5/dartmouth.html:text/html},
}

@online{aispring,
	title = {{AI} Spring? Four Takeaways from Major Releases in Foundation Models},
	url = {https://hai.stanford.edu/news/ai-spring-four-takeaways-major-releases-foundation-models},
	shorttitle = {{AI} Spring?},
	abstract = {As companies release new, more capable models, questions around deployment and transparency arise.},
	titleaddon = {Stanford {HAI}},
	urldate = {2023-11-16},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/IAJR8ZZY/ai-spring-four-takeaways-major-releases-foundation-models.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2023-11-16},
	date = {2012},
	file = {Full Text PDF:/Users/jonasprivat/Zotero/storage/3EWXCGHW/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@article{hinton_deep_2012,
	title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
	volume = {29},
	issn = {1558-0792},
	url = {https://ieeexplore.ieee.org/document/6296526},
	doi = {10.1109/MSP.2012.2205597},
	shorttitle = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
	abstract = {Most current speech recognition systems use hidden Markov models ({HMMs}) to deal with the temporal variability of speech and Gaussian mixture models ({GMMs}) to determine how well each state of each {HMM} fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over {HMM} states as output. Deep neural networks ({DNNs}) that have many hidden layers and are trained using new methods have been shown to outperform {GMMs} on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using {DNNs} for acoustic modeling in speech recognition.},
	pages = {82--97},
	number = {6},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
	urldate = {2023-11-16},
	date = {2012-11},
	note = {Conference Name: {IEEE} Signal Processing Magazine},
	file = {IEEE Xplore Abstract Record:/Users/jonasprivat/Zotero/storage/RAMEF63J/6296526.html:text/html},
}

@online{google_decade_2021,
	title = {A decade in deep learning, and what's next},
	url = {https://blog.google/technology/ai/decade-deep-learning-and-whats-next/},
	abstract = {Jeff Dean and Marian Croak of Google Research take a look at progress in {AI} and how Google has applied them in helpful ways, and look ahead to a responsible and inclusive path forward.},
	titleaddon = {Google},
	urldate = {2023-11-16},
	date = {2021-11-18},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/RR8MV6XH/decade-deep-learning-and-whats-next.html:text/html},
}

@online{house_2012_2019,
	title = {2012: A Breakthrough Year for Deep Learning},
	url = {https://medium.com/neuralmagic/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73},
	shorttitle = {2012},
	abstract = {Neural networks have been around for decades, with seminal early work pioneered by Geoffrey Hinton, Yann Lecun and others serving as major…},
	titleaddon = {Deep Sparse},
	author = {House, Bryan},
	urldate = {2023-11-16},
	date = {2019-07-17},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/SH25T85I/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73.html:text/html},
}

@online{openai_chatgpt,
	title = {{ChatGPT}},
	url = {https://chat.openai.com},
	abstract = {A conversational {AI} system that listens, learns, and challenges},
	urldate = {2023-11-16},
	langid = {american},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/LBX4TEGW/chat.openai.com.html:text/html},
}

@online{sitnflash_history_2017,
	title = {The History of Artificial Intelligence},
	url = {https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/},
	abstract = {by Rockwell Anyoha Can Machines Think? In the first half of the 20th century, science fiction familiarized the world with the concept of artificially intelligent robots. It began with the “heartless” Tin man from the Wizard of Oz and continued with the humanoid robot that impersonated Maria in Metropolis. By the 1950s, we had a generation of scientists, mathematicians, and philosophers with the concept of …},
	titleaddon = {Science in the News},
	author = {{SITNFlash}},
	urldate = {2023-11-16},
	date = {2017-08-28},
	langid = {american},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/XEA3S4EZ/history-artificial-intelligence.html:text/html},
}

@online{mooreslaw,
	title = {What Is Moore's Law and Is It Still True?},
	url = {https://www.investopedia.com/terms/m/mooreslaw.asp},
	abstract = {Moore's Law refers to Gordon Moore's perception that the number of transistors on a microchip doubles every two years, while the cost of computers is halved.},
	titleaddon = {Investopedia},
	urldate = {2023-11-16},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/7B94L9NR/mooreslaw.html:text/html},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2023-11-16},
	date = {1986-10},
	langid = {english},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
}

@article{burke_recommender_2011,
	title = {Recommender Systems: An Overview},
	volume = {32},
	rights = {Copyright (c)},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2361},
	doi = {10.1609/aimag.v32i3.2361},
	shorttitle = {Recommender Systems},
	abstract = {Recommender systems are tools for interacting with large and complex information spaces. They provide a personalized view of such spaces, prioritizing items likely to be of interest to the user. The field, christened in 1995, has grown enormously in the variety of problems addressed and techniques employed, as well as in its practical applications. Recommender systems research has incorporated a wide variety of artificial intelligence techniques including machine learning, data mining, user modeling, case-based reasoning, and constraint satisfaction, among others. Personalized recommendations are an important part of many on-line e-commerce applications such as Amazon.com, Netflix, and Pandora. This wealth of practical application experience has provided inspiration to researchers to extend the reach of recommender systems into new and challenging areas. The purpose of the articles in this special issue is to take stock of the current landscape of recommender systems research and identify directions the field is now taking. This article provides an overview of the current state of the field and introduces the various articles in the special issue.},
	pages = {13--18},
	number = {3},
	journaltitle = {{AI} Magazine},
	author = {Burke, Robin and Felfernig, Alexander and Göker, Mehmet H.},
	urldate = {2023-11-16},
	date = {2011-06-05},
	langid = {english},
	note = {Number: 3},
	file = {Full Text PDF:/Users/jonasprivat/Zotero/storage/9WB8FESM/Burke et al. - 2011 - Recommender Systems An Overview.pdf:application/pdf},
}

@online{google_howweuseai,
	title = {9 ways we use {AI} in our products},
	url = {https://blog.google/technology/ai/9-ways-we-use-ai-in-our-products/},
	abstract = {Here are nine ways we use {AI} today to make our products even more helpful.},
	titleaddon = {Google},
	urldate = {2023-11-16},
	date = {2023-01-19},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/LGW4B75B/9-ways-we-use-ai-in-our-products.html:text/html},
}

@online{elevenlabs,
	title = {{ElevenLabs}: {AI} Voice Generator \& Text to Speech},
	url = {https://elevenlabs.io/},
	urldate = {2023-11-16},
	file = {ElevenLabs\: AI Voice Generator & Text to Speech:/Users/jonasprivat/Zotero/storage/7BJNLSWQ/elevenlabs.io.html:text/html},
}

@online{midjourney,
	title = {Midjourney},
	url = {https://www.midjourney.com/home?callbackUrl=%2Fexplore},
	abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
	titleaddon = {Midjourney},
	urldate = {2023-11-16},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/TBDAE88X/home.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	rights = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program {AlphaGo} achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	pages = {484--489},
	number = {7587},
	journaltitle = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	urldate = {2023-11-16},
	date = {2016-01},
	langid = {english},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
}

@misc{silver_mastering_2017,
	title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	url = {http://arxiv.org/abs/1712.01815},
	doi = {10.48550/arXiv.1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single {AlphaZero} algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, {AlphaZero} achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	number = {{arXiv}:1712.01815},
	publisher = {{arXiv}},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2023-11-16},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1712.01815 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/jonasprivat/Zotero/storage/A2W8BFSK/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/jonasprivat/Zotero/storage/7IPJ8DST/1712.html:text/html},
}

@online{piper_ai_2019,
	title = {{AI} triumphs against the world’s top pro team in strategy game Dota 2},
	url = {https://www.vox.com/2019/4/13/18309418/open-ai-dota-triumph-og},
	abstract = {It’s the first time an {AI} has beat a world champion e-sports team.},
	titleaddon = {Vox},
	author = {Piper, Kelsey},
	urldate = {2023-11-16},
	date = {2019-04-13},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/X2DP6A2A/open-ai-dota-triumph-og.html:text/html},
}

@inreference{noauthor_artificial_2023-1,
	title = {Artificial neural network},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1183756198},
	abstract = {Artificial neural networks ({ANNs}, also shortened to neural networks ({NNs}) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An {ANN} is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.

Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.},
	booktitle = {Wikipedia},
	urldate = {2023-11-18},
	date = {2023-11-06},
	langid = {english},
	note = {Page Version {ID}: 1183756198},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/AALQL5PG/Artificial_neural_network.html:text/html},
}

@book{aggarwal_neural_2018,
	location = {Cham},
	title = {Neural Networks and Deep Learning: A Textbook},
	isbn = {978-3-319-94462-3 978-3-319-94463-0},
	url = {http://link.springer.com/10.1007/978-3-319-94463-0},
	shorttitle = {Neural Networks and Deep Learning},
	publisher = {Springer International Publishing},
	author = {Aggarwal, Charu C.},
	urldate = {2023-11-18},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-94463-0},
	keywords = {Adam, autoencoder, backpropagation, conjugate gradient-descent, Convolutional Neural Networks, Deep Learning, deep reinforcement learning, dropout, generative adversarial networks, Kohonean self-organizaing map, logistic regression, Machine Learning, Neural networks, perceptron, pretraining, Radial Basis Function Networks, Recurrent Neural Networks, Restricted Boltzmann Machines, {RMSProp}, word2vec},
	file = {Full Text PDF:/Users/jonasprivat/Zotero/storage/S82DKHBS/Aggarwal - 2018 - Neural Networks and Deep Learning A Textbook.pdf:application/pdf},
}

@online{gonfalonieri_understand_2020,
	title = {Understand Neural Networks \& Model Generalization},
	url = {https://towardsdatascience.com/understand-neural-networks-model-generalization-7baddf1c48ca},
	abstract = {The Challenge of Model Generalization, Overfitting and Regularization Methods for Deep Neural Networks},
	titleaddon = {Medium},
	author = {Gonfalonieri, Alexandre},
	urldate = {2023-11-18},
	date = {2020-01-29},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/78JNIBSX/understand-neural-networks-model-generalization-7baddf1c48ca.html:text/html},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	pages = {359--366},
	number = {5},
	journaltitle = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	urldate = {2023-11-18},
	date = {1989-01-01},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	file = {ScienceDirect Snapshot:/Users/jonasprivat/Zotero/storage/4J6YQPBP/0893608089900208.html:text/html},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep Learning},
	isbn = {978-0-262-03561-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	pagetotal = {800},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016-11-18},
}

@online{raschka_is_0000,
	title = {Is the logistic sigmoid function just a rescaled version of the hyberpolic tangent (tanh) function?},
	url = {https://sebastianraschka.com/faq/docs/tanh-sigmoid-relationship.html},
	abstract = {The short answer is: yes!},
	titleaddon = {Sebastian Raschka, {PhD}},
	author = {Raschka, Sebastian},
	urldate = {2023-11-18},
	date = {0000},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/THL6DDNC/tanh-sigmoid-relationship.html:text/html},
}

@online{bettilyon_computationalgraphs_2020,
	title = {Deep Neural Networks As Computational Graphs},
	url = {https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9},
	abstract = {{DNNs} Don’t Need To Be A Black Box},
	titleaddon = {Teb’s Lab},
	author = {Bettilyon, Tyler Elliot},
	urldate = {2023-11-23},
	date = {2020-05-05},
	langid = {english},
	file = {Snapshot:/Users/jonasprivat/Zotero/storage/3AKMBYX7/deep-neural-networks-as-computational-graphs-867fcaa56c9.html:text/html},
}

@book{haykin_neural_1998,
	location = {Upper Saddle River, {NJ}},
	edition = {Subsequent Edition},
	title = {Neural Networks: A Comprehensive Foundation: A Comprehensive Foundation: United States Edition},
	isbn = {978-0-13-273350-2},
	shorttitle = {Neural Networks},
	abstract = {For graduate-level neural network courses offered in the departments of Computer Engineering, Electrical Engineering, and Computer Science.Renowned for its thoroughness and readability, this well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. Thoroughly revised.},
	pagetotal = {842},
	publisher = {Pearson},
	author = {Haykin, Simon},
	date = {1998-07-01},
}

@misc{dubey_activation_2022,
	title = {Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark},
	url = {http://arxiv.org/abs/2109.14545},
	doi = {10.48550/arXiv.2109.14545},
	shorttitle = {Activation Functions in Deep Learning},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions ({AFs}), such as Logistic Sigmoid, Tanh, {ReLU}, {ELU}, Swish and Mish. In this paper, a comprehensive overview and survey is presented for {AFs} in neural networks for deep learning. Different classes of {AFs} such as Logistic Sigmoid and Tanh based, {ReLU} based, {ELU} based, and Learning based are covered. Several characteristics of {AFs} such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art {AFs} with different networks on different types of data. The insights of {AFs} are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/{ActivationFunctions}\}.},
	number = {{arXiv}:2109.14545},
	publisher = {{arXiv}},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	urldate = {2023-11-23},
	date = {2022-06-28},
	eprinttype = {arxiv},
	eprint = {2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/jonasprivat/Zotero/storage/BDK3PM37/Dubey et al. - 2022 - Activation Functions in Deep Learning A Comprehen.pdf:application/pdf;arXiv.org Snapshot:/Users/jonasprivat/Zotero/storage/ZKUGVVIA/2109.html:text/html},
}


@collection{rall_autodiff_1981,
	location = {Berlin, Heidelberg},
	title = {Automatic Differentiation: Techniques and Applications},
	volume = {120},
	isbn = {978-3-540-10861-0 978-3-540-38776-3},
	url = {http://link.springer.com/10.1007/3-540-10861-0},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Automatic Differentiation},
	publisher = {Springer},
	editor = {Rall, Louis B.},
	editorb = {Goos, G. and Hartmanis, J. and Brauer, W. and Brinch Hansen, P. and Gries, D. and Moler, C. and Seegmüller, G. and Stoer, J. and Wirth, N.},
	editorbtype = {redactor},
	urldate = {2023-11-30},
	date = {1981},
	doi = {10.1007/3-540-10861-0},
	keywords = {addition, Applications, computation, differential equation, Differentiation, equation, integration, Jacobi, mathematical programming, Nonlinear system, Numerical integration, online, optimization, software, techniques},
	file = {Submitted Version:/Users/jonasprivat/Zotero/storage/HDZB5CHQ/Rall - 1981 - Automatic Differentiation Techniques and Applicat.pdf:application/pdf},
}

@article{adams_backprop_autodiff_nodate,
	title = {Computing Gradients with Backpropagation},
	author = {Adams, Ryan P},
	langid = {english},
	file = {Adams - Computing Gradients with Backpropagation.pdf:/Users/jonasprivat/Zotero/storage/D3I3S8IR/Adams - Computing Gradients with Backpropagation.pdf:application/pdf},
}

@book{griewank_derivatives_2008,
	title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition},
	isbn = {978-0-89871-659-7},
	shorttitle = {Evaluating Derivatives},
	abstract = {Algorithmic, or automatic, differentiation ({AD}) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. This second edition covers recent developments in applications and theory, including an elegant {NP} completeness argument and an introduction to scarcity. There is also added material on checkpointing and iterative differentiation. To improve readability the more detailed analysis of memory and complexity bounds has been relegated to separate, optional chapters. The book consists of: a stand-alone introduction to the fundamentals of {AD} and its software; a thorough treatment of methods for sparse problems; and final chapters on program-reversal schedules, higher derivatives, nonsmooth problems and iterative processes.},
	pagetotal = {448},
	publisher = {{SIAM}},
	author = {Griewank, Andreas and Walther, Andrea},
	date = {2008-11-06},
	langid = {english},
	note = {Google-Books-{ID}: {qMLUIsgCwvUC}},
	keywords = {Computers / Computer Science, Computers / Computer Simulation, Computers / Programming / Algorithms, Mathematics / Linear \& Nonlinear Programming, Mathematics / Optimization, Science / Physics / Mathematical \& Computational},
}


