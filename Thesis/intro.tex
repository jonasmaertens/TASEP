\graphicspath{{img/intro/out}{img/intro}}

\chapter{Introduction}
\label{ch:intro}

\section{Artificial Intelligence}
\label{sec:artificial-intelligence}
Artificial Intelligence (AI) is a broad subject of study that can be defined in different ways~\cite[chapter 1]{russell_artificial_2021}.
John McCarthy, often called the “father of AI”~\cite{wiki_ai_2023, woo_fatherofai_2014, andresen_fatherofai_2002}, defines it as “the science and engineering of making intelligent machines”~\cite{stanford-whatisai}, where intelligence means “the computational part of the ability to achieve goals in the world”~\cite{stanford-whatisai}.
AI is sometimes mistakenly used interchangeably with Machine Learning.
Machine learning is a subset of AI concerned with enabling AI systems to learn from experience~\cite[chapter 1]{russell_artificial_2021}.
Machine learning enables the development of large-scale AI systems as they are used today.
\\
The study of artificial intelligence was first proposed by McCarthy et al. in late 1955 \cite{mccarthy_proposal_1955}.
It went through two major hype cycles in the sixties and the eighties~\cite{googlengram_ai, wiki_ai_2023, sitnflash_history_2017} followed by phases of “AI winter”.
The current (as of early 2024) AI boom, sometimes also called “AI spring”~\cite{aispring} was started by groundbreaking advances in speech recognition~\cite{hinton_deep_2012} and image classification~\cite{krizhevsky_imagenet_2012} in 2012~\cite{google_decade_2021, house_2012_2019} and reached the public at the latest in late 2022, following the release of ChatGPT~\cite{openai_chatgpt_intro}, a multipurpose AI-chatbot, open to everyone~\cite{openai_chatgpt}.
\\
These breakthroughs are made possible mainly by advancements in the field of machine learning, enabling AI systems to learn from huge amounts of data.
In addition, the exponential increase in computation and storage capabilities as predicted by Moore’s Law~\cite{mooreslaw}, algorithms like backpropagation~\cite{rumelhart_learning_1986} allowed incorporating large amounts of data into machine learning models in realistic amounts of time.
\\
Today, AI systems are indispensable in many areas such as web search engines~\cite{google_howweuseai}, recommendation systems~\cite{burke_recommender_2011}, human speech recognition and generation~\cite{elevenlabs, hinton_deep_2012}, image recognition and generation~\cite{midjourney, krizhevsky_imagenet_2012} and personal assistants~\cite{openai_chatgpt_intro} and surpasses humans in high level strategy games like go and chess~\cite{silver_mastering_2016, silver_mastering_2017} as well as other video games~\cite{piper_ai_2019}.

\chapter{Neural Networks}
\label{ch:neural-networks}
\section{Overview}
\label{sec:overview}
At the heart of almost all the technologies mentioned in the last paragraph are deep artificial neural networks.
The next section will outline the mathematical details of how these systems work and learn.
Although the comparison of artificial neural networks, from now on just called “neural networks”, to their biological counterpart can be criticized as oversimplifying the inner workings of biological brains~\cite[chapter 1.1]{aggarwal_neural_2018}, the architecture of neural networks is heavily inspired by how decision-making and learning work in the human brain~\cite[chapter 1.1]{aggarwal_neural_2018, mit_nnexplained}.
I will illustrate the basic principles of neural networks at the example of a network that detects the gender of a person by looking at pictures.
\\
\\
A neural network consists of a number of layers of artificial neurons, called \textit{nodes}.
In a \textit{fully connected} network, each node is connected to every node in the next layer.
The connection strengths are called \textit{weights}.
An image of a person can be fed into the network by setting the \textit{activation} values of the first layer of the network, the \textit{input layer}, to the individual pixel values of the image.
This information is then fed forward through the layers of the network until the \textit{output layer} is reached.
If a network has at least one layer in between the input and output layer, it is called a \textit{deep neural network}.
These intermediate layers are called \textit{hidden layers}.
If the outputs of each layer are only connected to the inputs of the next layer, the network is called a \textit{feedforward neural network}.
If \textit{feedback} connections are allowed, the network is called a \textit{recurrent neural network}.
\\
In our example, the output layer should consist of only two nodes.
If the activation of the first node is larger than the activation of the second node, the network thinks that the person in the picture is a male.
If on the other hand the second node has a larger activation, the network classifies this person as female~\cite[chapter 1.2]{aggarwal_neural_2018}.
\\
\\
In order to make accurate predictions, a reasonable set of network parameters (i.e.the weights) has to be found.
This is done by training the network with pre-classified images.
After an image has been processed by the network, the output is compared to the correct classification and the network parameters are updated in a way that would improve the networks output if the same image was to be processed again~\cite[chapter 1.2]{aggarwal_neural_2018,ibm_nn}.
\\
This is similar to how humans learn from experience.
If we were to misclassify a persons gender, the unpleasant social experience that may come with that mistake would cause us to update our internal model of what different genders look like to not make the same mistake again.
\\
\\
One of the main strengths of neural networks is their ability to \textit{generalize}~\cite{gonfalonieri_understand_2020}.
When a network was trained on a large enough set of examples, it gains the ability to generalize this knowledge to examples that were previously unseen.
The gender classification network from our example doesn't just memorize the genders of the people it has seen, but instead learns about the features that help to identify the gender of a random person.
\\
\\
The problem of image classification is a rather complex one.
One wouldn't typically think of it as finding a function that maps the values of each input pixel to the classification output.
But even very complex problems can be modeled by equally complex functions.
The universal approximation theorem states, that a feedforward neural network with at least one hidden layer with appropriate activation functions (see~\ref{subsec:activation-functions} for details) can approximate any continuous function if given enough nodes~\cite[chapter 6.4.1]{goodfellow_deep_2016}.
That's why training a neural network can be thought of as fitting the network to the training data.
\\
\section{Mathematical Details}
\label{sec:nn-mathematical-details}
The following section will outline the mathematical details of how neural networks work.
The definitions and derivations are based on~\cite[chapter 1.2-1.3]{aggarwal_neural_2018},~\cite[chapter 5-6]{goodfellow_deep_2016},~\cite{ibm_nn} as well as~\cite[chapter 4.4]{haykin_neural_1998}.
\\
\\
The most complete treatment of the mathematical details of neural networks is arguably given by the framework of computational graphs~\cite{bettilyon_computationalgraphs_2020}.
In this framework, a neural network is treated as single that maps a set of input values to a set of output values.
This function is composed of individual mathematical operations and can be represented as a directed graph \cite[section 1.4]{haykin_neural_1998}.
I will not rigorously define the framework of computational graphs here, as this is beyond the scope of this thesis.
Instead, I will explain the principles of neural networks starting from a single neuron and then build up to a fully connected neural network.
\subsection{Notation}
\label{subsec:nn-notation}
The following notation will be used throughout this section:
\begin{equation*}
    \begin{array}{ll}
    \bm{x} & : \text { input vector of the neural network} \\
    \bm{a} & : \text { activation vector of a layer} \\
    \bm{z} & : \text { pre-activation vector of a layer} \\
    \bm{y} & : \text { output vector of the neural network} \\
    \bm{b} & : \text { bias vector of a layer} \\
    x_i, a_i, z_i, y_i, b_i & : \text { individual elements of the respective vectors, for individual nodes } \\
    \bm{w_i} & : \text { weight vector of all weights connected to neuron i} \\
    w_{ij} & : \text { weight of the connection from neuron i of a layer to neuron j of the previous layer } \\
    W & : \text { weight matrix of a layer. Contains rows $\bm{w_i}$} \\
    ^{[J]} & : \text { superscript denoting the layer of a variable} \\
    \end{array}
\end{equation*}
\subsection{Single Node}
\label{subsec:single-neuron}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{single_neuron}
    \caption{A single node of a neural network. To get the activation $a$ of the node, the pre-activation $z$ is calculated from the inputs $a_i$ and the bias $b$ and is passed through an activation function $\phi$.}
    \label{fig:single-neuron}
\end{figure}
Before we can build a neural network out of nodes, we have to define how a single node works.
Each node receives the activations $a_i$ from the nodes of the previous layer as inputs.
Each connection is assigned a weight $w_i$, stored in the node's weight vector $\bm{w}$.
\\
Additionally, each node has a so-called \textit{bias} $b$.
The bias shifts the net input of the node by a constant value.
It is needed to model certain problems where part of the prediction is independent of the input \cite[6]{aggarwal_neural_2018}.
Examples include all problems where the output should not be zero even if all inputs are zero.
\\
\\
The net input, called \textit{pre-activation value} $z$ of a node is the weighted sum of all inputs plus the bias:
\begin{equation}
    z = \sum_{i=1}^{m} w_i a_i + b = \bm{w} \cdot \bm{a} + b \text{.}
    \label{eq:pre-activation}
\end{equation}
The pre-activation value is then passed through an \textit{activation function} $\phi$ to get the activation $a$ of the node, which is then passed on to the next layer, where the process is repeated:
\begin{equation}
    a = \phi(z) \text{.}
    \label{eq:activation}
\end{equation}
The whole process is illustrated in figure \ref{fig:single-neuron}.

\subsection{Activation Functions}
\label{subsec:activation-functions}
The activation function $\phi$ is used to introduce non-linearity into the network and thus increasing its modeling power \cite[section 1.2.1.3]{aggarwal_neural_2018}.
Some activation functions are also referred to as \textit{squashing function}~\cite[10]{haykin_neural_1998}, as they map the unbounded pre-activation value $z$ to a bounded activation value $a$.
The choice of activation function has a large impact on the performance of the network in terms of both accuracy and speed \cite{dubey_activation_2022}.
The type of function heavily influences the way that information is processed by the network and the complexity of the function naturally has a large impact on the computational cost of the network.
The best choice therefore depends on the problem that is being solved and the architecture of the network.
Typically, the same activation function is used for all nodes in a layer and is applied to the pre-activation value of each node individually, but different layers can use different activation functions depending on their purpose \cite[174]{goodfellow_deep_2016}.
For a long time, the most popular activation functions were (\cite[chapter 6.3]{goodfellow_deep_2016}, \cite[section 1.2.1.3]{aggarwal_neural_2018}) the sigmoid function:
\begin{equation}
    \phi(z) = \sigma(z) = \frac{1}{1+e^{-z}}
    \label{eq:sigmoid}
\end{equation}
and the hyperbolic tangent function:
\begin{equation}
    \phi(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1
    \label{eq:tanh}
\end{equation}
as well as the sign function:
\begin{equation}
    \phi(z) = \text{sign}(z) = \begin{cases}
        1 & \text{if } z > 0 \\
        0 & \text{if } z = 0 \\
        -1 & \text{if } z < 0 \text{.}
    \end{cases}
    \label{eq:sign}
\end{equation}
The sign function can map neural network output to a binary classification, but it is not suitable for backpropagation (see section \ref{subsec:backprop}) due to its derivative being zero almost everywhere.
The sigmoid function and the hyperbolic tangent function are both differentiable and limit the output to the range $(-1, 1)$ and $(0, 1)$ respectively.
They are however more computationally expensive than other activation functions and suffer from the \textit{vanishing gradient problem}.
The vanishing gradient problem is caused by the fact that the derivative of the sigmoid function approaches zero for large absolute values of $z$.
This leads to the weights of the nodes in the first layers of the network being updated very slowly, as the gradient of the loss function with respect to the weights of these nodes is very small (see section \ref{subsec:backprop})\cite[section 1.4.2]{aggarwal_neural_2018}\cite{dubey_activation_2022}.
The sigmoid function and the hyperbolic tangent function can be seen in figure \ref{fig:sigmoid} and \ref{fig:tanh}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{sigmoid}
        \caption{The sigmoid activation function.}
        \label{fig:sigmoid}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{tanh}
        \caption{The hyperbolic tangent activation function.}
        \label{fig:tanh}
    \end{subfigure}
    \caption{The most popular activation functions before the rise of ReLU.}
    \label{fig:tanhsigmoid}
\end{figure}
\\
\\
In recent years, the \textit{rectified linear unit} (ReLU) and similar stepwise linear functions have become the go-to activation functions for deep neural networks~\cite[chapter 6.3.2]{goodfellow_deep_2016}\cite{dubey_activation_2022}.
ReLU is defined as:
\begin{equation}
    \phi(z) = \max(0, z) = \begin{cases}
        0 & \text{if } z \leq 0 \\
        z & \text{if } z > 0 \text{.}
    \end{cases}
    \label{eq:relu}
\end{equation}
Its main advantage is its very low computational cost, as it consists of only a single comparison.
Although it is not as prone to the vanishing gradient problem as the sigmoid and the hyperbolic tangent, the problem still exists for negative values of $z$.
This has been addressed by variations like \textit{Leaky ReLU}, introducing a small but non-zero slope for negative values~\cite{dubey_activation_2022}:
\begin{equation}
    \phi(z) = \max(0.01 z, z) = \begin{cases}
        0.01 z & \text{if } z \leq 0 \\
        z & \text{if } z > 0 \text{.}
    \end{cases}
    \label{eq:leaky-relu}
\end{equation}
The ReLU function and its leaky version can be seen in figures \ref{fig:relu} and \ref{fig:leaky-relu}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{relu}
        \caption{The rectified linear unit (ReLU) activation function.}
        \label{fig:relu}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{leaky_relu.pdf}
        \caption{The leaky rectified linear unit (Leaky ReLU) activation function.}
        \label{fig:leaky-relu}
    \end{subfigure}
    
    \caption{Modern, stepwise linear activation functions.}
    \label{fig:subfigures}
\end{figure}


\subsection{Forward-Propagation in Feedforward Neural Networks}
\label{subsec:forward-propagation}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{nn}
    \caption{A feedforward neural network with one hidden layer.}
    \label{fig:nn}
\end{figure}
Now that the workings of the nodes are defined, we can build a fully connected neural network out of these building blocks.
I will illustrate the process at the example of a feedforward neural network as defined in section \ref{sec:overview}.
An example of such a network can be seen in figure \ref{fig:nn}.
\\
\\
The process of feeding an input vector $\bm{x}$ through the network to get the output vector $\bm{y}$ is called \textit{forward-propagation}, as the information propagates through the network layer.
The activation of any layer $\bm{a}^{[J]}$ can be calculated from the activation of the previous layer $\bm{a}^{[J-1]}$ by generalizing equation \ref{eq:pre-activation} and \ref{eq:activation} to the vector case:
\begin{equation}
    z_k^{[J]} = \sum_{i=1}^{m} w_{ki}^{[J]} a_i^{[J-1]} + b_k^{[J]} \implies \bm{z}^{[J]} = W^{[J]} \bm{a}^{[J-1]} + \bm{b}^{[J]} \text{,}
    \label{eq:pre-activation-vector}
\end{equation}
\begin{equation}
    a_k^{[J]} = \phi(z_k^{[J]}) \implies \bm{a}^{[J]} = \phi(\bm{z}^{[J]}) \text{,}
    \label{eq:activation-vector}
\end{equation}
where the activation function $\phi: \mathbb{R}^m \rightarrow \mathbb{R}^m$ is applied element-wise.
The activation of the input layer $\bm{a}^{[0]}$ is simply the input vector $\bm{x}$ and the activation of the output layer $\bm{a}^{[L]}$ is the output vector $\bm{y}$.
\\
\\
Equations \ref{eq:pre-activation-vector} and \ref{eq:activation-vector} can be applied recursively to calculate the activation of each layer from the input layer to the output layer:
\begin{align}
    \bm{y} = \bm{a}^{[L]} &= \phi(W^{[L]} \bm{a}^{[L-1]} + \bm{b}^{[L]}) \\
    &= \phi(W^{[L]} \phi(W^{[L-1]} \bm{a}^{[L-2]} + \bm{b}^{[L-1]}) + \bm{b}^{[L]}) \\
    &= \phi(W^{[L]} \phi(W^{[L-1]} \phi(\dots \phi(W^{[1]} \bm{x} + \bm{b}^{[1]}) \dots) + \bm{b}^{[L-1]}) + \bm{b}^{[L]}) \text{.}
    \label{eq:forward-propagation}
\end{align}
This equation also shows the significance of the activation function.
Without it, the whole network would be equivalent to a single layer and could be replaced by a single matrix multiplication.
It would therefore only be able to model linear functions.
To show this, let's set $\phi(z)$ to the identity in equation \ref{eq:forward-propagation}.
This yields:
\begin{align}
    \bm{y} &= W^{[L]} W^{[L-1]} \dots W^{[1]} \bm{x} + \bm{b}^{[L]} + W^{[L]} \bm{b}^{[L-1]} + \dots + W^{[L]} W^{[L-1]} \dots W^{[2]} \bm{b}^{[1]} \\
    &= \widetilde{W} \bm{x} + \tilde{\bm{b}} \text{.}
    \label{eq:linear-network}
\end{align}

\subsection{Loss Functions and Gradient Descent}
\label{subsec:loss-functions}
In order to produce meaningful output, the network's weights and biases have to be adjusted to minimize the error of the network's output.
This process is called \textit{training} the network.
The error of the network is measured by a \textit{loss function} $\lambda(\bm{y}, \bm{\hat{y}})$, where $\bm{\hat{y}}$ is some target output vector.
The loss function is a measure of how far the network's output $\bm{y}$ is from the target output $\bm{\hat{y}}$.
As we want to minimize the network's error, the training process is essentially an optimization problem \cite[chapter 4.3]{goodfellow_deep_2016}.
Let's step back from neural networks for a moment and look at a method to minimize a function $f(\bm{x})$ with respect to its parameters $\bm{x}$.
The most common method to do this in machine learning is \textit{gradient descent} \cite[chapter 4.3]{goodfellow_deep_2016}.
\\
\\
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{gradient_descent_small_lr}
        \caption{Gradient descent with a small learning rate.}
        \label{fig:gradient-descent-small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{gradient_descent_large_lr}
        \caption{Gradient descent with a large learning rate.}
        \label{fig:gradient-descent-large}
    \end{subfigure}
    \caption{Gradient descent with different learning rates. Choosing the learning rate too small can lead to slow convergence and to being stuck in local minima. Choosing the learning rate too large can lead to \textit{overshooting} and to the algorithm diverging. Here, the algorithm still converges, but the large oscillations slow down the convergence.}
    \label{fig:gradient-descent}
\end{figure}
If we imagine the function $f(\bm{x})$ as a landscape, the goal of gradient descent is to find the lowest point of the landscape.
To do this, the algorithm starts at some point $\bm{x}_0$ and in each iteration, it takes a step in the direction of the steepest descent.
The size of the step is determined by the \textit{learning rate} $\eta$.
Choosing an appropriate learning rate is crucial for the algorithm to converge.
Figure \ref{fig:gradient-descent} shows gradient descent with different learning rates and the resulting paths through the landscape.
\\
\\
As is known from multivariable calculus, the direction of the steepest descent of a function $f(\bm{x})$ is given by the negative gradient $\nabla f(\bm{x})$.
We can therefore update the parameters $\bm{x}$ in each iteration by:
\begin{equation}
    \bm{x}_{n+1} = \bm{x}_n - \eta \nabla f(\bm{x}_n) \text{.}
    \label{eq:gradient-descent}
\end{equation}
After a sufficient number of iterations, the algorithm will converge to a local minimum of the function.
In the region around the minimum, the gradient is close to zero and the algorithm will not change the parameters significantly anymore.
We therefore define a threshold $\epsilon$ and stop the algorithm if the norm of the gradient falls below this threshold.
The gradient descent algorithm is summarized in algorithm \ref{alg:gradient-descent}.
\begin{algorithm}
    \caption{Gradient Descent}
    \label{alg:gradient-descent}
    \begin{algorithmic}[1]
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \Require
            \Statex $f(\bm{x})$: Function to minimize
            \Statex $\eta$: Learning rate
            \Statex $\epsilon$: Threshold
            \Statex $\bm{x}_0$: Initial parameters
        \Ensure $\bm{x^*}=\arg\min f(\bm{x})$: Parameters that minimize $f(\bm{x})$
        \output{$\bm{x\star}=\arg\min f(\bm{x})$: Parameters that minimize $f(\bm{x})$}
        \Statex
        \State $\bm{x} \gets \bm{x}_0$ \Comment{Initialize parameters}
        \While{$\norm{\nabla f(\bm{x}_n)} > \epsilon$} \Comment{Until convergence}
            \State $\bm{x} \gets \bm{x} - \eta \nabla f(\bm{x})$ \Comment{Update parameters}
        \EndWhile
        \State \Return $\bm{x}$ \Comment{Return parameters}
    \end{algorithmic}
\end{algorithm}

Going back to neural networks, the function that we want to minimize is the loss function $\lambda(\bm{y}, \bm{\hat{y}})$.
As the loss function depends on the network's output $\bm{y}$, which in turn depends on the network's parameters $\bm{w}$ and $\bm{b}$, the loss function is a function of the network's parameters $\lambda(\bm{w}, \bm{b})$.
\\
\\
For single layer networks, the calculation of the gradients is straightforward.
For multi-layer networks however, the loss function depends on the parameters of earlier layers in a non-trivial way.
The next section will outline an algorithm, that allows the efficient calculation of the gradients of multi-layer networks.

\subsection{The Backpropagation Algorithm}
\label{subsec:backprop}
The calculation of the gradients is the most computationally expensive part of training a neural network.
The invention of the \textit{backpropagation} algorithm by Rumelhart et al. in 1986~\cite{rumelhart_learning_1986} was a major breakthrough in the field of neural networks, as it allowed very efficient gradient calculation and thus enabled the training of large neural networks.
\\
The idea behind backpropagation is to propagate the error back through the network after each forward-propagation step using \textit{local gradients} $\delta$.
During the forward-propagation step, the activations of each layer are stored in memory and used to calculate the derivatives need for the backpropagation step.
This is a form of \textit{automatic differentiation}~\cite{adams_backprop_autodiff_nodate,rall_autodiff_1981} and allows the calculation of the gradients with the same time complexity as the forward-propagation step.
This is sometimes called the \textit{cheap gradient principle}~\cite{griewank_derivatives_2008}.
The following mathematical derivation of the backpropagation algorithm is based on~\cite[chapter 4.4]{haykin_neural_1998} as well as~\cite[chapter 6.5]{goodfellow_deep_2016}.
\\
\\
To calculate the update of a weight $w_{ij}^{[K]}$, we have to calculate the derivative of the loss function $\lambda$ with respect to the weight $w_{ij}^{[K]}$:
\begin{equation}
    \Delta w_{ij}^{[K]} = \eta \cdot \frac{\partial \lambda}{\partial w_{ij}^{[K]}} \text{.}
    \label{eq:weight-derivative} 
\end{equation}
This derivative can be evaluated using the chain rule:
\begin{equation}
    \frac{\partial \lambda}{\partial w_{ij}^{[K]}} = \frac{\partial \lambda}{\partial a_i^{[K]}} \frac{\partial a_i^{[K]}}{\partial z_i^{[K]}} \frac{\partial z_i^{[K]}}{\partial w_{ij}^{[K]}} = \delta_i^{[K]} a_j^{[K-1]} 
    \label{eq:chain-rule}
\end{equation}
where we used equation \ref{eq:pre-activation-vector} to simplify the last derivative and defined the local gradient $\delta_j^{[K]}$ as:
\begin{equation}
    \delta_i^{[K]} = \frac{\partial \lambda}{\partial a_i^{[K]}} \frac{\partial a_i^{[K]}}{\partial z_i^{[K]}} \text{.}
    \label{eq:local-gradient}
\end{equation}
Let's take a closer look at the local gradient $\delta_i^{[K]}$.
The second factor in equation \ref{eq:local-gradient} is the derivative of the activation function $\phi$ with respect to the pre-activation value $z_i^{[K]}$.
This term is straightforward to calculate.
Remembering that the activation function is applied element-wise, using equation \ref{eq:activation-vector} we get:
\begin{equation}
    \frac{\partial a_i^{[K]}}{\partial z_i^{[K]}} = \frac{\partial \phi(z_i^{[K]})}{\partial z_i^{[K]}} = \phi'(z_i^{[K]}) \text{.}
    \label{eq:activation-derivative}
\end{equation}
The first factor in equation \ref{eq:local-gradient} is the derivative of the loss function $\lambda$ with respect to the activation $a_i^{[K]}$.
If layer $K$ is the output layer, this derivative is simply the derivative of the loss function with respect to the output value $y_i$:
\begin{equation}
    \frac{\partial \lambda}{\partial a_i^{[K]}} = \frac{\partial \lambda}{\partial y_i} \text{.}
    \label{eq:loss-derivative-output}
\end{equation}
If layer $K$ is not the output layer, the derivative is slightly more complicated.
We can use the chain rule trick again, re-introducing the activation values $a_n^{[K+1]}$ of the next layer, which all depend on $a_i^{[K]}$:
\begin{equation}
    \frac{\partial \lambda}{\partial a_i^{[K]}} = \sum_{n=1}^{m_{K+1}} \frac{\partial \lambda}{\partial a_n^{[K+1]}} \frac{\partial a_n^{[K+1]}}{\partial a_i^{[K]}} = \sum_{n=1}^{m_{K+1}} \frac{\partial \lambda}{\partial a_n^{[K+1]}} \frac{\partial a_n^{[K+1]}}{\partial z_n^{[K+1]}} \frac{\partial z_n^{[K+1]}}{\partial a_i^{[K]}} \text{.}
    \label{eq:loss-derivative-hidden}
\end{equation}
Similar to equation \ref{eq:chain-rule}, we can use equation \ref{eq:pre-activation-vector} to simplify the last derivative and identify the local gradients $\delta_n^{[K+1]}$: 
\begin{equation}
    \frac{\partial \lambda}{\partial a_i^{[K]}} = \sum_{n=1}^{m_{K+1}} \delta_n^{[K+1]} w_{ni}^{[K+1]} \text{.}
    \label{eq:loss-derivative-hidden-simplified}
\end{equation}
Plugging equations \ref{eq:activation-derivative} and \ref{eq:loss-derivative-output} or \ref{eq:loss-derivative-hidden-simplified} back into equation \ref{eq:local-gradient} yields the final form of the local gradient:
\begin{equation}
    \delta_i^{[K]} = \phi'(z_i^{[K]}) \cdot \begin{cases}
         \frac{\partial \lambda}{\partial y_i} & \text{if layer } K \text{ is the output layer} \\
         \sum_{n=1}^{m_{K+1}} \delta_n^{[K+1]} w_{ni}^{[K+1]} & \text{otherwise} \text{.}
    \end{cases}
    \label{eq:local-gradient-final}
\end{equation}
This equation can be applied recursively to calculate the local gradients of all layers from the output layer to the input layer.
The weight update $\Delta w_{ij}^{[K]}$ can then be calculated using equation \ref{eq:weight-derivative} and \ref{eq:local-gradient-final}:
\begin{equation}
    \Delta w_{ij}^{[K]} = \eta \cdot \delta_i^{[K]} a_j^{[K-1]} \text{.}
    \label{eq:weight-update}
\end{equation}
When introducing the backpropagation algorithm, I summarized it as a method of \textit{propagating the error back through the network}.
The recursive usage of equation \ref{eq:local-gradient-final} is exactly that: We start at the output layer and calculate the local gradients of all nodes in the output layer using equation \ref{eq:local-gradient-final}.
Then we use these local gradients to calculate the local gradients of the previous layer using equation \ref{eq:local-gradient-final} again and so on until we reach the input layer.
Note that we need to store the activations of each layer in memory during the \textit{forward-pass} to calculate the updates during the \textit{backward-pass} as mentioned in the beginning of this section.
\\
\\
The derivations for the bias updates are analogous to the weight updates and are therefore omitted here.
The only difference is that the last term in equation \ref{eq:chain-rule} is replaced by $1$ as the bias is not connected to any previous layer.
The algorithm for a whole training step (i.e. one forward-pass and one backward-pass) is summarized in algorithm \ref{alg:backpropagation}.
\begin{algorithm}
    \caption{One training step of a neural network: Forward- and Backpropagation}
    \label{alg:backpropagation}
    \begin{algorithmic}[1]
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \Require
            \Statex $\bm{x}$: Input vector
            \Statex $\bm{\hat{y}}$: Target output vector
            \Statex $\bm{w}$: Weight array containing all weight matrices $\bm{w}^{[K]}$
            \Statex $\bm{b}$: Bias array containing all bias vectors $\bm{b}^{[K]}$
            \Statex $\lambda$: Loss function
            \Statex $\phi$: Activation function
            \Statex $\eta$: Learning rate
        \Ensure Updates the weights $\bm{w}$ and biases $\bm{b}$ of the network in place.
        \Statex
        \State $\bm{a}^{[0]} \gets \bm{x}$ \Comment{Initialize activations}
        \For{$K = 1, \dots, L$} \Comment{Forward-pass}
            \State $\bm{z}^{[K]} \gets \bm{w}^{[K]} \bm{a}^{[K-1]} + \bm{b}^{[K]}$ \Comment{Calculate pre-activations}
            \State $\bm{a}^{[K]} \gets \phi(\bm{z}^{[K]})$ \Comment{Calculate activations}
        \EndFor
        \State $\delta^{[L]} \gets \phi'(\bm{z}^{[L]}) \cdot \frac{\partial \lambda(\bm{a}^{[L]}, \bm{\hat{y}})}{\partial \bm{a}^{[L]}}$ \Comment{Initialize local gradients}
        \For{$K = L, \dots, 1$} \Comment{Backward-pass}
            \State $\Delta \bm{w}^{[K]} \gets \eta \cdot \delta^{[K]} \bm{a}^{[K-1]}$ \Comment{Calculate weight updates}
            \State $\Delta \bm{b}^{[K]} \gets \eta \cdot \delta^{[K]}$ \Comment{Calculate bias updates}
            \State $\delta^{[K-1]} \gets \phi'(\bm{z}^{[K-1]}) \cdot \bm{w}^{[K]T} \delta^{[K]}$ \Comment{Calculate local gradients}
        \EndFor
        \State $\bm{w} \gets \bm{w} - \Delta \bm{w}$ \Comment{Update weights}
        \State $\bm{b} \gets \bm{b} - \Delta \bm{b}$ \Comment{Update biases}
    \end{algorithmic}
\end{algorithm}

\subsection{Summary}
\label{subsec:nn-summary}
In this chapter, the foundations of neural networks were outlined.
Section~\ref{subsec:single-neuron} started by defining the building blocks of neural networks: The \textit{nodes}.
Section~\ref{subsec:forward-propagation} combined \textit{Layers} of these nodes into a \textit{fully connected neural network}.
The specific \textit{architecture} of the network, that is the number of layers, the number of nodes per layer and the \textit{activation functions}, depends on the problem that is being solved.
Later, we will see that in our case of Deep-Q-Learning, the size of the \textit{input vector} is determined by the number of cells that are visible to the agent and the size of the \textit{output vector} is determined by the number of possible actions that the agent can take.
\\
\\
On this basis, the \textit{forward-propagation} step could be analyzed in detail and the significance of the activation function was explained.
Next, section~\ref{subsec:loss-functions} introduced the \textit{loss function} as a measure of the network's error and the \textit{gradient descent} algorithm was outlined as a method to minimize this error.
Finally, the \textit{backpropagation} algorithm treated in section~\ref{subsec:backprop} provides a way to efficiently compute the gradients needed for gradient descent and enables the \textit{training} of large neural networks.
The whole process of forward-propagation, followed by backpropagation and network parameter updates is summarized in pseudocode in algorithm~\ref{alg:backpropagation}.





\chapter{Deep Q-Learning}
\label{ch:dql}


\section{Reinforcement Learning}
Out of the three main branches of machine learning, \textit{supervised learning}, \textit{unsupervised learning} and \textit{reinforcement learning}, reinforcement learning is the most similar to the way humans learn.
\\
While supervised learning is based on the idea of learning from examples and unsupervised learning is concerned with finding patterns in data \cite{ibm_machine_learning}, reinforcement learning is based on the idea of learning from experience \cite[chapter 1.1]{sutton_reinforcement_nodate}.
In reinforcement learning, the learning agent is not told what action to take, but instead has to learn which actions lead to a desired outcome by trial and error.
\\
This framework has led to many successes over the last decades, including computer programs that beat the world's best players in board games like chess, Go, backgammon \cite{silver_mastering_2016,silver_mastering_2017,tesauro_temporal_1995} or even in video games like Atari games \cite{mnih_playing_2013} and Dota 2 \cite{openai_dota_2019}, as well as teaching robots how to walk and handle objects \cite{kober_reinforcement_2013,openai_learning_2019}.
\textcolor{red}{TODO: Add OpenAI $Q^*$ here?}

\subsection{Overview}
In a reinforcement learning scenario, the \textit{agent} observes some information about the \textit{environment}, called the \textit{state} of the environment.
It then uses its \textit{policy} to map the state to an \textit{action} that it takes in the environment.
As a reaction, the environment returns a \textit{reward} to the agent and transitions to a new state \cite{openai_spinning_up_rl_intro}.
\\
During the learning process, the agent tries to find a policy that maximizes the total reward that it receives from the environment.
Depending on the type of problem, the agent can either try to maximize the reward in the short term or in the long term.
\\
\\
The following sections will refine these concepts and introduce the formalism of reinforcement learning.

\subsection{Notation}
\label{subsec:notation}
Before diving into the details of reinforcement learning, I will introduce the notation that will be used throughout the following sections.
It is mostly identical to the notation used in \cite{sutton_reinforcement_nodate}.
Some of these definitions might seem a bit abstract at first, but they will become clearer in the following sections, where they are introduced and explained one by one.

\begin{itemize}
    \item $s_t$: The state of the environment at time step $t$.
    \item $a_t$: The action taken by the agent at time step $t$, based on the state $s_t$.
    \item $r_t$: The reward returned by the environment at time step $t$, based on the state $s_t$, action $a_t$ and the new state $s_{t+1}$.
    \item $\pi$: The policy of the agent, mapping states to actions.
    \item $\pi(a|s)$: The probability of the agent taking action $a$ in state $s$.
    \item $p(s_{t+1} | s_t, a_t)$: The probability of the environment transitioning to state $s_{t+1}$ after the agent takes action $a_t$ in state $s_t$.
    \item $\tau$: A trajectory, i.e. a sequence of states, actions and rewards $(s_0, a_0, r_0, s_1, a_1, r_1, \dots)$.
    \item $G_t$: The return at time step $t$, i.e. the total discounted reward from time step $t$ onwards.
    \item $\gamma$: The discount factor, determining how much future rewards are worth compared to immediate rewards.
    \item $v_\pi(s)$: The value of state $s$ under policy $\pi$, i.e. the expected return when starting in state $s$ and following policy $\pi$.
    \item $q_\pi(s, a)$: The value of taking action $a$ in state $s$ under policy $\pi$, i.e. the expected return when starting in state $s$, taking action $a$ and then following policy $\pi$.
    \item $\pi_*, v_*, q_*$: The optimal policy, value function and action-value function respectively.
\end{itemize}


\subsection{Important Concepts}

\subsubsection{Markov Decision Processes}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{agent_env_interface.pdf}
    \caption{The agent-environment interface in a Markov decision process as defined in \cite[chapter 3.1]{sutton_reinforcement_nodate}. The agent observes the state $s_t$ of the environment and takes an action $a_t$. The environment transitions to a new state $s_{t+1}$ and returns a reward $r_{t}$ to the agent.}
    \label{fig:agent-env-interface}
\end{figure}
Markov decision processes (MDPs) are a mathematical framework for modeling decision-making in situations where outcomes are partly random and only partly under the control of the agent \cite[chapter 3]{sutton_reinforcement_nodate}.
In an MDP, the agent's actions influence not only the immediate reward, but also the next state of the environment and therefore all future rewards.
They are a powerful abstraction that can be used to model a wide range of problems, from simple board games to complex real-world scenarios.
\\
MDPs can be seen as a generalization of discrete-time \textit{Markov chains}.
Markov chains are a special case of \textit{stochastic processes}, where the next state of the system is determined only by the current state and not by any previous states.
This property is called the \textit{Markov property} and can be expressed mathematically as \cite{serfozo_markov_2009}:
\begin{equation}
    p(s_{t+1} | s_1, \dots, s_t) = p(s_{t+1} | s_t) = p_{ss'} \text{.}
    \label{eq:markov-property}
\end{equation}
The probability $P$ of transitioning to state $s_{t+1}$ from state $s_t$ is independent of the previous states $s_1, \dots, s_{t-1}$ and only depends on the current state $s_t$.
It is intuitive that the state trajectories in a reinforcement learning scenario should always satisfy this property, as it allows the agent to make decisions based on the current state without having to consider the whole history of states that led to the current state.
\\
\\
MDPs slightly modify the definition of Markov chains by introducing the notion of actions and rewards.
As outlined in the previous section, the agent interacts with the environment by taking actions $a_i$ based on the current state of the environment $s_i$ and receives rewards $r_i$ from the environment in return.
This \textit{agent-environment interface} is illustrated in figure \ref{fig:agent-env-interface}.
A \textit{trajectory} $\tau$ in an MDP is a sequence of states, actions and rewards \cite[48]{sutton_reinforcement_nodate}:
\begin{equation}
    \tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \dots) \text{.}
    \label{eq:trajectory}
\end{equation}
The transition probability now depends on the action $a_t$ that the agent takes in state $s_t$:
\begin{align}
    p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} &\rightarrow [0, 1] \nonumber \\
    s_{t+1}, r_t, s_t, a_t &\mapsto p(s_{t+1}, r_t | s_t, a_t) \text{,}
    \label{eq:transition-probability}
\end{align}
where $\mathcal{S}$ denotes the set of all possible states and $\mathcal{A}$ denotes the set of all possible actions.\\
This probability function completely determines the dynamics of the reinforcement learning environment.
Environments can be entirely deterministic or entirely stochastic, or anything in between.
The four-argument transition probability, if known, can of course be used to calculate other properties of the environment, such as the three-argument state-transition probability \cite[49]{sutton_reinforcement_nodate}:
\begin{align}
    p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} &\rightarrow [0, 1] \nonumber \\
    s_{t+1}, s_t, a_t &\mapsto p(s_{t+1} | s_t, a_t) = \sum_{r_t \in \mathcal{R}} p(s_{t+1}, r_t | s_t, a_t) \text{.}
    \label{eq:state-transition-probability}
\end{align}
where $\mathcal{R}$ denotes the set of all possible rewards.\\
Furthermore, knowing the four-argument transition probability of an environment allows us to create a simulation of the environment, which can be used to test reinforcement learning algorithms.
This is a very useful property, as it allows us to test reinforcement learning algorithms in a controlled environment before deploying them in the real world.\\

\subsubsection{Reward}
After each time step $t$, the agent receives a reward $r_t$ from the environment.
The reward is a scalar value that indicates how good or bad the action $a_t$ that the agent took in state $s_t$ was \cite[53]{sutton_reinforcement_nodate}.
Rewards are the \enquote{trainer's} way of telling the agent what it should achieve.
In order to use the full potential of reinforcement learning, rewards should be chosen carefully.
The reward signal should not tell the agent \textit{how} to achieve the goal, but only \textit{what} the goal is.
The agent should then be able to figure out the best way to achieve the goal by itself \cite[ch. 3.2]{sutton_reinforcement_nodate}.
For example, if the goal is to teach a robot to walk, the agent should receive a reward for moving forward while maintaining a certain balance, but not for moving its legs in a certain way.
\\
In order to be able to learn from these rewards, not just the immediate reward $r_t$ should be taken into account, but also the rewards that the agent will receive in the future.
This is done by introducing the notion of \textit{return}
The return $G_t$ is a measure of the cumulative reward that the agent will receive from time step $t$ onwards \cite[ch. 3.3]{sutton_reinforcement_nodate}.
The simplest form of return is just the sum of all future rewards:
\begin{equation}
    G_t = r_t + r_{t+1} + r_{t+2} + \dots + r_{T} = \sum_{i=t}^{T} r_{i} \text{,}
    \label{eq:return}
\end{equation}
where $T$ is the final time step in the current \textit{episode}.
Episodic tasks are task where a \textit{terminal state} $s_T$ can be reached, after which the episode ends.
Examples are board games like chess or Go, where the game ends after one player wins.
In contrast, for \textit{continuing tasks}, as is the case for the environment that we will consider in this thesis, there is no terminal state and $T=\infty$.
In this case, it's useful to introduce a \textit{discounted} return \cite[ch. 3.3]{sutton_reinforcement_nodate}:
\begin{equation}
    G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots = \sum_{i=t}^{\infty} \gamma^{i-t} r_{i} \text{.}
    \label{eq:discounted-return}
\end{equation}
The \textit{discount factor} $\gamma$ determines how much immediate rewards should be valued compared to future rewards.
A discount factor of $\gamma=0$ means that only the immediate reward is taken into account, while a discount factor of $\gamma=1$ would value all future rewards equally.
\\
The discounted reward formula can be rewritten recursively as \cite[55]{sutton_reinforcement_nodate}:
\begin{align}
    G_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots \nonumber \\
    &= r_t + \gamma (r_{t+1} + \gamma r_{t+2} + \dots) \nonumber \\
    &= r_t + \gamma G_{t+1} \text{.}
    \label{eq:discounted-return-recursive}
\end{align}
This recursive formulation is very important, as it allows us to express the return in terms of the return at the next time step.
This will be useful in the next section, where we want to calculate the expected return of a state-action pair.\\


\subsubsection{Policies}
\label{subsubsec:policies}
Reinforcement learning agents are characterized by their \textit{policy} $\pi$.
The policy maps states $s$ to actions $a=\pi(s)$ and therefore determines the behavior of the agent \cite[ch. 3.5]{sutton_reinforcement_nodate}.
The policy can be either \textit{deterministic} or \textit{stochastic}.
A deterministic policy maps each state to exactly one action, while a stochastic policy maps each state to a probability distribution over actions.
If a policy deterministically maps states to what it believes to be the best action, it is called a \textit{greedy} policy \cite[64]{sutton_reinforcement_nodate}.
If an optimal policy has been found, it can be greedy, as it will always choose the best action.
However, during the learning process, stochastic policies should be preferred, as they allow the agent to explore the environment and find better policies.
In order to learn, an \enquote{inexperienced} agent needs to try actions that it assumes to be suboptimal to find out whether they are really suboptimal.
This is called the \textit{exploration-exploitation dilemma} \cite[3]{sutton_reinforcement_nodate}.\\
A popular solution which will also be used in this thesis is the \textit{$\epsilon$-greedy} policy \cite[100]{sutton_reinforcement_nodate}.
$\epsilon$-greedy policies are greedy with probability $1-\epsilon$ and choose a random action with probability $\epsilon$.
During training, $\epsilon$ is slowly decreased over time, so that the agent will explore the environment more at the beginning and exploit its knowledge later on.
During evaluation, $\epsilon$ is set to zero, so that the agent will always choose the best action.\\


\subsubsection{Value Functions}
To actually build a policy that maximizes the expected return, the agent needs to know how good each state or state-action pair is. In order to do this, most reinforcement learning algorithms use \textit{(state) value functions} \cite[ch. 3.5]{sutton_reinforcement_nodate}. Value functions estimate the expected return of a state $s$ when following policy $\pi$ \cite[58]{sutton_reinforcement_nodate}:
\begin{equation}
    v_\pi(s) = \mathbb{E}_\pi[G_t | s_t = s] = \mathbb{E}_\pi \left[ \sum_{i=t}^{\infty} \gamma^{i-t} r_{i} \middle| s_t = s \right] \text{.}
    \label{eq:value-function}
\end{equation}
In the same manner, we can define the \textit{action-value function} \cite[58]{sutton_reinforcement_nodate}:
\begin{equation}
    q_\pi(s, a) = \mathbb{E}_\pi[G_t | s_t = s, a_t = a] = \mathbb{E}_\pi \left[ \sum_{i=t}^{\infty} \gamma^{i-t} r_{i} \middle| s_t = s, a_t = a \right] \text{.}
    \label{eq:action-value-function}
\end{equation}


\subsubsection{Optimality and Bellman Equations}
Now the recursion formula from equation \ref{eq:discounted-return-recursive} comes in handy to express the value functions in terms of the value functions of the next state:
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi[G_t | s_t = s] \nonumber \\
    &= \mathbb{E}_\pi[r_t + \gamma G_{t+1} | s_t = s] \nonumber \\
    &= \sum_a \pi(a|s) \sum_{s'} \sum_{r_t} p(s', r_t | s, a) \left[ r_t + \gamma \mathbb{E}_\pi[G_{t+1} | s_{t+1} = s'] \right] \nonumber \\
    &= \sum_a \pi(a|s) \sum_{s'} \sum_{r_t} p(s', r_t | s, a) \left[ r_t + \gamma v_\pi(s') \right] \text{,}
    \label{eq:bellman-expectation}
\end{align}
where we reintroduced the four-argument transition probability. The summations are over all possible actions $a$ and all possible next states $s'$ and rewards $r_t$. Equation \ref{eq:bellman-expectation} is called the \textit{Bellman equation} for $v_\pi$ \cite[59]{sutton_reinforcement_nodate}.\\
The Bellman equation for $q_\pi$ is analogous \cite{openai_spinning_up_rl_intro}:
\begin{equation}
    q_\pi(s, a) = \sum_{s'} \sum_{r_t} p(s', r_t | s_t, a_t) \left[ r_t + \gamma \sum_{a_{t+1}} \pi(a_{t+1}|s_{t+1}=s') q_\pi(s', a_{t+1}) \right] \text{.}
    \label{eq:bellman-expectation-action-value}
\end{equation}
The Bellman equations heavily simplify the calculation of the value functions, as they allow us to express the value of a state or state-action pair in terms of the values of the next state or state-action pairs.
Imagine a game of chess: Even for very fast computers, evaluating every possible trajectory of moves until one player wins is not feasible.
However, the Bellman equations allow us to calculate the value of a state or state-action pair without having to consider all possible trajectories.
We just have to keep track of the values of individual states or state-action pairs and update them according to the Bellman equations.
\\
\\
We can now use these concepts to define \textit{optimal} policies $\pi_*$. A policy is considered \enquote{better} than another policy, if it achieves a higher expected return in every state \cite[62]{sutton_reinforcement_nodate}:
\begin{equation}
    \pi \geq \pi' \Leftrightarrow v_\pi(s) \geq v_{\pi'}(s) \quad \forall s \in \mathcal{S} \text{.}
    \label{eq:policy-ordering}
\end{equation} 
An optimal policy $\pi_*$ is a policy that is better than or equal to all other policies \cite[62]{sutton_reinforcement_nodate}. Associating the value functions $v_*$ and $q_*$ with the optimal policy $\pi_*$, we can define the optimal (action-)value functions as \cite[ch. 3.6]{sutton_reinforcement_nodate}:
\begin{equation}
    v_*(s) = \max_\pi v_\pi(s) \quad \forall s \in \mathcal{S} \text{.}
    \label{eq:optimal-value-function}
\end{equation}
\begin{equation}
    q_*(s, a) = \max_\pi q_\pi(s, a) \quad \forall s \in \mathcal{S}, a \in \mathcal{A} \text{.}
    \label{eq:optimal-action-value-function}
\end{equation}
The optimal value functions satisfy the \textit{Bellman optimality equations} \cite[ch. 3.6]{sutton_reinforcement_nodate}. When an agent follows an optimal policy, the sum over all possible actions can be replaced by using the action that maximizes the expected return:
\begin{align}
    q_*(s, a) &= \mathbb{E}_\pi \left[ r_t + \gamma \max_{a'} q_*(s_{t+1}, a') \middle| s_t = s, a_t = a \right] \nonumber \\
    &= \sum_{s'} \sum_{r_t} p(s', r_t | s, a) \left[ r_t + \gamma \max_{a'} q_*(s', a') \right] \text{.}
    \label{eq:bellman-optimality-action-value}
\end{align}
Solving equation \ref{eq:bellman-optimality-action-value} yields the optimal action-value function $q_*$, which can then be used to derive the optimal policy $\pi_*$. For most reinforcement learning problems however, it is not feasible to solve the Bellman optimality equations analytically, even if the transition probabilities are known. Therefore, most reinforcement learning algorithms use iterative methods to approximate the optimal value functions \cite[ch. 4]{sutton_reinforcement_nodate}.


\subsection{Summary}
Before we dive into the details of the Deep Q-Learning algorithm, let's summarize the most important concepts of reinforcement learning that were introduced in this section.
\\
\\
Reinforcement learning is a framework for \textit{learning from experience}.
An \textit{agent} interacts with an \textit{environment} by taking \textit{actions} based on the current \textit{state} of the environment.
The environment returns a \textit{reward} which the agent uses to keep track of the \textit{return} that it will receive in the future.
This in turn allows the agent to learn a \textit{policy} that maximizes the expected the return by learning the \textit{value} of each state or state-action pair.
\textit{Optimal} policies can be found by solving the \textit{Bellman optimality equations}.



\subsection{Algorithms}
\label{subsec:rl-algorithms}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{openai_algorithms}
    \caption{Overview of the different classes of reinforcement learning algorithms \cite{openai_spinning_up_rl_part2}. The papers that introduced the algorithms are also available at \cite{openai_spinning_up_rl_part2}.}
    \label{fig:rl-algorithms}
\end{figure}
%model based vs model free
%on-policy vs off-policy
%value based vs policy based
Figure \ref{fig:rl-algorithms} provides a basic overview of the different classes of reinforcement learning algorithms. We will not explain the details of each algorithm here, but instead focus on the general distinctions between the different classes of algorithms.
\\
\\
The first distinction is between \textit{model-based} and \textit{model-free} algorithms. Model-based algorithms have an internal model of the environment, which is either learned or provided by the user \cite{openai_spinning_up_rl_part2}. This allows them to plan ahead and simulate the environments dynamics to find the best action to take. Model-free algorithms on the other hand only implicitly learn the dynamics of the environment by interacting with it and learning value functions, as explained in the previous sections. Model-free algorithms are more flexible, as they can be applied to any environment, but they are also less sample-efficient, as they have to learn the dynamics of the environment by trial and error. Model-based algorithms on the other hand can be more sample-efficient, but they are harder to implement and fine-tune to specific problems \cite{openai_spinning_up_rl_part2}. Probably the most famous model-based reinforcement learning algorithm is AlphaZero \cite{silver_mastering_2017}, a program that taught itself to play chess, Go and Shogi at superhuman levels.
\\
\\
Model-free algorithms can be distinguished further by \textit{what} is learned. In the last section, we introduced the concept of value functions. Algorithms that learn value functions are called \textit{value-based} algorithms. If they learn the action-value function $q_\pi$, they are called \textit{Q-learning} algorithms. Most of these algorithms use \textit{off-policy} learning, which means that they learn the value of the optimal policy $\pi_*$ while following a different policy $\pi$ \cite{openai_spinning_up_rl_part2}. This allows them to learn from data that was collected at a previous stage of the training process, making them very sample-efficient. Probably the most famous Q-learning algorithm is Deep Q-Learning, which we will discuss in detail in the next section.
\\
The other class of model-free algorithms are \textit{policy-based} algorithms. These algorithms directly learn the optimal policy $\pi_*$, instead of learning value functions. Most of these algorithms use \textit{on-policy} learning, restricting them to only learn from data that was collected while following the current policy $\pi$ \cite{openai_spinning_up_rl_part2}. Because of that, they are less sample-efficient than Q-learning algorithms, but they also tend be more stable, as they directly learn the policy, instead of indirectly learning the policy by learning value functions \cite{openai_spinning_up_rl_part2}. 
\\
\\
As we can see in figure \ref{fig:rl-algorithms}, there are also algorithms that use ideas from different classes of algorithms. In general, it is hard to draw clear distinctions between the different classes of algorithms, as their modular nature allows them to be combined in many different ways \cite{openai_spinning_up_rl_part2}.
\\
\\
Choosing the best algorithm for a specific problem is one of the most important steps in applying reinforcement learning to a real-world problem. In this thesis, we will use Deep-Q-Learning, as its use of deep neural networks allows it to scale and generalize well. It is also a model free, off-policy algorithm, which makes it relatively easy to implement and modify. The sampling efficiency of off-policy learning is also a big advantage, as it allows training the network quickly on normal hardware. 



\section{Q-Learning}
\label{sec:q-learning}
Before diving into the details of the Deep Q-Learning algorithm, we will introduce the general concepts of Q-Learning, building on the concepts of reinforcement learning that were introduced in the previous section.
\\
\\
Q-Learning was introduced by Watkins in 1989 \cite{watkins_learning_1989}. It is a model-free, off-policy algorithm that learns an action-value function $Q(s,a)$ which is guaranteed to converge to the optimal action-value function $q_*$ \cite{watkins_q-learning_1992} \cite[ch. 6.5]{sutton_reinforcement_nodate}
\\
The algorithm works by storing a table of action-values $Q(s,a)$ for each state-action pair $(s,a)$ that the agent has encountered. Such a \textit{Q-table} can be seen in figure \ref{fig:q-table}. In the beginning, the Q-table is initialized randomly. Then, the agent interacts with the environment by taking actions and receiving rewards. The policy used for this interaction is derived from the Q-table for example by choosing the action with the highest action-value in an $\epsilon$-greedy manner. For each sample (state $s$, action $a$, reward $r$, next state $s'$) that the agent encounters, the Q-table is updated according to the following formula \cite[ch. 6.5]{sutton_reinforcement_nodate}:
\begin{equation}
    Q(s, a) \gets Q(s, a) + \eta \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \text{,}
    \label{eq:q-learning-update}
\end{equation}
where $\eta$ is the learning rate and $\gamma$ is the discount factor. Comparing with equation \ref{eq:bellman-optimality-action-value}, we can see that this update rule is an iterative approximation of the Bellman optimality equation for $q_*$. The new approximation of the optimal action-value is a weighted sum of the old approximation, and the new information that was gained from the sample, i.e. the immediate reward $r$ and the discounted value of the next action. The learning rate $\eta$ determines how much the new information is weighted compared to the old approximation. The discount factor again $\gamma$ determines how much future rewards are valued compared to immediate rewards.
\\
\\
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \node
            (q-table-before)
            [draw]
            {
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                     & $a_1$ & $a_2$ & $\dots$ & $a_n$ \\
                    \hline
                    $s_1$ & $Q(s_1, a_1)$ & $Q(s_1, a_2)$ & \dots & $Q(s_1, a_n)$ \\
                    \hline
                    $s_2$ & $Q(s_2, a_1)$ & $Q(s_2, a_2)$ & \dots & $Q(s_2, a_n)$ \\
                    \hline
                    $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
                    \hline
                    $s_m$ & $Q(s_m, a_1)$ & $Q(s_m, a_2)$ & \dots & $Q(s_m, a_n)$ \\
                    \hline
                \end{tabular}
            };
    \end{tikzpicture}
    \caption{A Q-table for an environment with $m$ states and $n$ actions.}
    \label{fig:q-table}
\end{figure}
We see that while time complexity stays constant when using lookup tables, the space complexity grows linearly with the number of states and actions. This makes Q-learning impractical for environments with large state spaces. Also, environments that have continuous state or action spaces have to be discretized, which can lead to a loss of information and therefore suboptimal policies. Even for seemingly simple environments such as a grid, where each grid cell can either be occupied or unoccupied, the number of possible states grows exponentially with the size of the grid. Therefore, we need a more efficient way to represent the action-values. This is where Deep Q-Learning comes in.

\section{Deep Q-Learning}
\label{subsec:deep-q-learning}
To solve the problem of large state spaces, we look back at chapter \ref{ch:neural-networks}, where we learned, that deep neural networks are universal function approximators. This makes them a good candidate for approximating the action-values $Q(s,a)$. When we replace the Q-table with a neural network, we supply the current state $s$ as input and get the action-values $Q(s,a)$ as output. For our grid environment example from the last section, this would mean that we supply the current grid as input. This eliminates the exponential space complexity of the Q-table, as the input vector of the network now grows linearly with the number of cells. Furthermore, the discretization problem is also solved, as the network can now take continuous inputs. Ideally, the network would store the same information as the table, but in a more compact way by learning the patterns in the data.  
\\
\\
This approach is called \textit{Deep Q-Learning} (DQN) and was introduced by Mnih et al. in 2013 \cite{mnih_playing_2013}. DQN also introduces a few other concepts apart from the Q-network, which we will discuss in the following sections.

\subsection{Q-Network}
\label{subsec:q-network}
The Q-network, sometimes also called \textit{policy network}, was already discussed in the previous section. It is a neural network that replaces the Q-table and approximates the action-values $Q(s,a)$. During evaluation, the environments state is supplied as input to the network. After a forward pass through the network, as explained in section \ref{subsec:forward-propagation}, the output vector of the network corresponds to the approximate action-values $Q(s,a)$ for each action $a$. When the network has converged to a good approximation of the optimal action-values $q_*$, the action with the highest action-value can be chosen as the action that the agent takes in the current state. During training, this action is chosen in an $\epsilon$-greedy manner, as explained in section \ref{subsubsec:policies} to maintain exploration \cite{mnih_human-level_2015}. 

\subsection{Target Network}
\label{subsec:target-network}
The Q-network also changes the way that the parameters are updated.
We can no longer update individual Q-values, as this information is now encoded in the weights and biases of the network.
Instead, we have to update the parameters of the network.
This is done by using  \textit{gradient descent} and the \textit{backpropagation} algorithm, as explained in sections \ref{subsec:loss-functions} and \ref{subsec:backprop}.
\\
\\
The loss function that is used for the gradient descent requires a target output vector $\hat{y}$ for each sample. In the case of Q-learning, this would be the optimal action-values $q_*(a)$. As this is what we're optimizing for, we can't use it as the target output vector. Instead, we could use an earlier approximation of the action-values $Q(s,a)$. However, this can lead to oscillations or divergence of the network parameters, as the target network would use the same parameters as the Q-network \cite{mnih_human-level_2015}. To solve this problem, DQN introduces a second network, called the \textit{target network}. The target network is a clone of the Q-network with frozen parameters. Using this separate network to calculate the action-value targets makes the learning process more stable \cite{mnih_human-level_2015}.
\\
In the original algorithm, the target network parameters are updated periodically by copying the parameters from the Q-network. This is called \textit{hard target network update} \cite{mnih_human-level_2015}. In 2016, Lillicrap et al. \cite{lillicrap_continuous_2015} introduced \textit{soft target network updates}, where the target network parameters are updated by slowly blending the parameters of the Q-network into the target network parameters:
\begin{equation}
    \theta_{\text{target}} \gets \tau \theta_{\text{Q}} + (1 - \tau) \theta_{\text{target}} \text{,}
    \label{eq:soft-target-update}
\end{equation}
where $\tau\ll1$ is a small number between 0 and 1, $\theta_{\text{target}}$ are the target network parameters and $\theta_{\text{Q}}$ are the Q-network parameters.
\\
This makes the learning process even more stable, as the target network parameters are constrained to slow updates \cite{lillicrap_continuous_2015}. In this thesis, we will also use soft target network updates.

\subsection{Experience Replay}
\label{subsec:experience-replay}
In the beginning of this section, DQN was introduced as an off-policy algorithm. Mnih et al. introduced a technique called \textit{experience replay} \cite{mnih_human-level_2015}, which massively improves the algorithms' quality. The technique is inspired by the way that humans learn \cite{mnih_human-level_2015} and works by storing tuples of states, actions, rewards and next states $(s_t, a_t, r_t, s_{t+1})$, called \textit{transitions} in a \textit{replay buffer}. During training, the network is updated by sampling a batch of transitions from the replay buffer and performing a gradient descent step on the loss function. This not only allows the network to learn from the same experience multiple times, increasing sample efficiency, but also breaks up the correlation between consecutive samples, which would otherwise decrease learning efficiency and stability \cite{mnih_human-level_2015}. In order to prevent the replay buffer from growing infinitely, the oldest samples are discarded when the buffer is full. This total buffer size and the sampling batch size are hyperparameters that have to be tuned for each problem. 
\\
\\
% problem rare transitions -> prioritized experience replay
One problem that exists in both on-policy learning and off-policy learning with experience replay is that rare transitions are sampled very infrequently, which can slow down learning. To solve this problem, Schaul et al. introduced \textit{prioritized experience replay} in 2015 \cite{schaul_prioritized_2016}. In prioritized experience replay, each transition is assigned a priority based on the magnitude of the loss function \cite{schaul_prioritized_2016}. That way, transitions that are \enquote{surprising} to the agent are sampled more frequently, which speeds up learning \cite{schaul_prioritized_2016}.

\subsection{Summary}
In this section, we learned that Deep Q-Learning uses a neural network to map states to action-values. This allows it to scale to large and continuous state spaces. Learning is performed \textit{offline}, by saving encountered transitions to a replay buffer and sampling batches of transitions from the buffer to update the network parameters. A separate target network is used to calculate the action-value targets, which makes the learning process more stable. The target network parameters are updated by slowly blending the parameters of the Q-network into the target network parameters. The complete algorithm is shown in algorithm \ref{alg:dqn}.

\begin{algorithm}[h]
    \caption{Deep Q-Learning}
    \label{alg:dqn}
    \begin{algorithmic}
        \State Initialize replay buffer $\mathcal{D}$ to capacity $N$
        \State Initialize Q-network $Q$ with parameters $\theta$
        \State Initialize target network $\hat{Q}$ with parameters $\theta^-=\theta$
        \State (Initialize environment) 
        \State Initialize state $s_1$
        \State Initialize $\epsilon$ to $\epsilon_0$
        \For{steps $t=1,\dots,T$}
            \State $\triangleright$ Generate new transition
            \State Perform forward pass through $Q$ to get action-values $Q(s_t, a')$ 
            \State With probability $\epsilon$ select a random action $a_t$, otherwise select $a_t = \arg\!\max_a Q(s_t, a)$
            \State Execute action $a_t$ in environment and observe reward $r_t$ and next state $s_{t+1}$
            \State Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$
            \State $\triangleright$ Update Q-network parameters
            \State Sample random minibatch of transitions $(\bm{s}, \bm{a}, \bm{r}, \bm{s'})$ from $\mathcal{D}$
            \State Set $\bm{y} = \bm{r} + \gamma \max_{a'} \hat{Q}(\bm{s'}, a')$
            \State Calculate mean loss $\mathcal{L} = \text{mean}(\mathcal{L}(\bm{y}, Q(\bm{s}, \bm{a})))$
            \State Perform gradient descent step on $\mathcal{L}$ using backpropagation and update $\theta$
            \State $\triangleright$ Update target network parameters
            \State Update target network parameters $\theta^- \gets \tau \theta + (1 - \tau) \theta^-$
            \State $\triangleright$ Update exploration rate
            \State Decrease $\epsilon$   
        \EndFor
    \end{algorithmic}
\end{algorithm}

\chapter{Physical Model}
% (t)asep is a lattice model, where partcles move on lattice
% TASEP = totally asymmetric simple exclusion process
% totally asymmetric = particles can only move in one direction
% simple = particles can only move to the next site
% exclusion = each site can only be occupied by one particle
% process = stochastic process, markov process
% first introduced by macdonald gibbs and pipkin in 1968, as model for protein synthesis, where particles are ribosomes and sites are codons
% introduced in mathematics by spitzer in 1970
% 1D: 1D lattice, particles have prob p to move to the right, prob 1-p to move to the left, if site is occupied, particle stays, each particle has its own clock. tasep when only to the right. either periodic or different rates for insertion and removal of particles, exact solutions are known and different phases can be found, depending on these rates / density of particles

% 2d model used in this thesis is totally asymmetric in one direction, symmetric in the other direction
% particles can only move to the right, up, down or stay
% periodic boundary conditions in both directions = constant number of particles, like on a torus
% simplified model for traffic flow or similar transport processes, for example movement of motor proteins on microtubules
% 2d tasep is not well studied, no exact solutions known, only approximations

% we will use smarticles, smart particles, that have a neural network to decide where to move based on their surroundings. framed as reinforcement learning problem, reward structure is defined by the goal of the system, which is to maximize transport. 

% section structure
% 1D TASEP
% 2D TASEP
% Smarticles

In this chapter, we will introduce the physical model that that will be used in this thesis. We will start by introducing the 1D TASEP, which is a well studied model for transport processes. Then, we will introduce the 2D TASEP, which is the model that we will use in this thesis. Finally, we will introduce the concept of smarticles, which are smart particles that use reinforcement learning to maximize transport in the 2D TASEP.

\section{1D TASEP}
\label{sec:1d-tasep}
The TASEP is one of the most well studied models in non-equilibrium statistical physics. It can be used as a stochastic model for transport processes on a 1D lattice and was first introduced by MacDonald, Gibbs and Pipkin in 1968 \cite{macdonald_kinetics_1968} as a model for protein synthesis, where particles are ribosomes and sites are codons. It was later introduced in mathematics by Spitzer in 1970 \cite{spitzer_interaction_1970}. The name TASEP stands for \textit{totally asymmetric simple exclusion process}. \textit{Totally asymmetric} means that particles can only move in one direction. In the more general ASEP, particles have different rates $p$ and $q$ for jumping to the left or right respectively. \textit{Simple} means that particles can only move to the next site. \textit{Exclusion} means that each site can only be occupied by one particle. \textit{Process} means that the model is a stochastic process, specifically a continous-time Markov process on the finite state space
\begin{equation}
    \mathcal{S} = \{0, 1\}^L \text{,}
    \label{eq:state-space}
\end{equation}
where $L$ is the number of sites. Each site can either be occupied by a particle or empty, so the state space consists of all possible configurations of particles on the lattice. It can be treated as a continous-time process, as each particle moves according to its own internal clock, although in this thesis we will discretize time by picking a random particle in each time step.
\\
\\
The 1D TASEP can be treated with periodic boundary conditions, where the first and last site are connected as shown in figure \ref{fig:tasep_1d_periodic}, or with different rates for insertion and removal of particles, as shown in figure \ref{fig:tasep_1d_finite}. Exact solutions are known for the one-dimensional TASEP and different phases can be found, depending on these rates or the density of particles \cite{schutz_exact_1997,blythe_nonequilibrium_2007}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{tasep_a.pdf}
    \caption{A 1D TASEP with periodic boundary conditions. The number of particles is constant. Each particle has a probability $p$ to move clockwise and a probability $1-p$ to move counterclockwise. If the target site is occupied, the particle stays.}
    \label{fig:tasep_1d_periodic}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{tasep_b.pdf}
    \caption{A 1D finite TASEP. The reservoir on the left inserts particles with rate $\alpha$ and the reservoir on the right removes particles with rate $\beta$.}
    \label{fig:tasep_1d_finite}
\end{figure}



\section{2D TASEP}
\label{sec:2d-tasep}
The TASEP can be generalized to higher dimensions. In this thesis, we will use the 2D TASEP, which can be used as a simplified model for traffic flow with multiple lanes or intracellular transport processes, for example the movement of motor proteins on microtubules. In this case, the process is totally asymmetric in one direction and symmetric in the other direction. Particles can only move to the right (\enquote{forward}), up or down. We will use periodic boundary conditions in both directions, which means that the number of particles is constant, like on a torus. A small 8x4 version of the 2D TASEP is shown in figure \ref{fig:tasep_2d}. The 2D TASEP is not as well studied as the 1D TASEP. No exact solutions are known and only approximations, such as mean-field theory, exist \cite{goykolov_asymmetric_2007}.
\\
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{tasep_2d.pdf}
    \caption{A 2D TASEP with periodic boundary conditions. The number of particles is constant. Each particle has a probability $p$ to move clockwise and a probability $1-p$ to move counterclockwise. If the target site is occupied, the particle stays.}
    \label{fig:tasep_2d}
\end{figure}
In this thesis, we will increase the complexity of the 2D TASEP by introducing velocities. The velocity is implemented as a probability of actually performing an attempted jump. For example, if a particle's velocity is 0.5, it will only move in 50\% of the cases where a jump is attempted and the target site is empty. 
\\
\\
After a basic numerical treatment of the classical 2D TASEP, we will try to optimize the total transport in these systems by introducing \textit{smarticles}. 

\section{Smarticles}
\label{sec:smarticles}
Smarticles (\textbf{Smart} Part\textbf{icles}) are a form of active matter as described in section TODO. In this thesis we will use smarticles to optimize the transport in the 2D TASEP and observe how global structures can emerge from local interactions. The smarticles will be implemented as particles with a neural network that decides where to move based on the surroundings. 
\\
\\
This can be framed as a reinforcement learning problem, where the reward structure is defined by the goal of the system, which is to maximize transport. Local interactions can also be integrated into the reward structure in order to bias the learned behavior towards certain global structures.  


\chapter{Implementation}
\label{ch:implementation}
Now that the theoretical foundations of the TASEP and Deep Q-Learning have been introduced, we can start to put the pieces together and implement the smarticles. This chapter will start with the implementation of the classical 2D TASEP to set a baseline for the performance of the smarticles. Then, we will implement the smart TASEP and introduce a number of modifications. 
\\



\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{uml.pdf}
    \caption{UML diagram of the SmartTasep python package. Only selected methods and attributes are shown. The full documentation is available at \cite{maertens_smarttasep_github_2023}.}
    \label{fig:implementation}
\end{figure}







